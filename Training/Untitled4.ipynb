{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os         \n",
    "from random import shuffle \n",
    "from tqdm import tqdm      \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "TRAIN_DIR = 'train'\n",
    "IMG_SIZE = 100\n",
    "LR = 1e-3\n",
    "MODEL_NAME = 'dogs-vs-cats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label(image_name):\n",
    "   \n",
    "    word_label = image_name.split('.')[-3]\n",
    "    if word_label == 'cat':\n",
    "        return np.array([1,0])\n",
    "    elif word_label == 'dog':\n",
    "        return np.array([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        path = os.path.join(TRAIN_DIR, img)\n",
    "        img_data = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))\n",
    "        training_data.append([np.array(img_data), create_label(img)])\n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1715/1715 [00:03<00:00, 467.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data= create_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train_data[:-100]\n",
    "test=train_data[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_train = [i[1] for i in train]\n",
    "X_test = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_test = [i[1] for i in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1615, 100, 100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1615, 2, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_Y_one_hot = to_categorical(y_train)\n",
    "print(X_train.shape)\n",
    "train_Y_one_hot.shape\n",
    "#Y_test = to_categorical(Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D/Relu:0\", shape=(?, 100, 100, 32), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"model.fit({'input': X_train}, {'targets': y_train}, n_epoch=10, \\n          validation_set=({'input': X_test}, {'targets': y_test}), \\n          snapshot_step=500, show_metric=True, run_id=MODEL_NAME) \""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "print(convnet)\n",
    "\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "convnet = fully_connected(convnet, 2, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "model = tflearn.DNN(convnet, tensorboard_dir='log', tensorboard_verbose=0)\n",
    "\"\"\"model.fit({'input': X_train}, {'targets': y_train}, n_epoch=10, \n",
    "          validation_set=({'input': X_test}, {'targets': y_test}), \n",
    "          snapshot_step=500, show_metric=True, run_id=MODEL_NAME) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "num_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1615, 100, 100, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0,len(y_train)):\n",
    "    if(y_train[x][0]==1):\n",
    "        y_train[x]=1;\n",
    "    else:\n",
    "        y_train[x]=0;\n",
    "for x in range(0,len(y_test)):\n",
    "    if(y_test[x][0]==1):\n",
    "        y_test[x]=1;\n",
    "    else:\n",
    "        y_test[x]=0;\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1615, 2), (1615, 100, 100, 1), (100, 2), (100, 100, 100, 1))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)\n",
    "y_train.shape,X_train.shape,y_test.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "\"\"\"binary_model = Sequential()\n",
    "binary_model.add(Conv2D(16, kernel_size=(32, 5),activation='linear',input_shape=(50,50,1),padding='same'))\n",
    "binary_model.add(LeakyReLU(alpha=0.1))\n",
    "binary_model.add(MaxPooling2D((3, 5),padding='same'))\n",
    "binary_model.add(Dropout(0.2))\n",
    "binary_model.add(Conv2D(32, (64, 5), activation='linear',padding='same'))\n",
    "binary_model.add(LeakyReLU(alpha=0.1))\n",
    "binary_model.add(MaxPooling2D(pool_size=(3, 5),padding='same'))\n",
    "binary_model.add(Dropout(0.2))\n",
    "binary_model.add(Flatten())\n",
    "binary_model.add(Dense(128, activation='linear'))\n",
    "binary_model.add(LeakyReLU(alpha=0.1))                  \n",
    "binary_model.add(Dense(num_classes, activation='softmax'))\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(100, 100, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#binary_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1615 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "1615/1615 [==============================] - ETA: 38s - loss: 4.5590 - acc: 0.50 - ETA: 29s - loss: 5.2867 - acc: 0.56 - ETA: 26s - loss: 5.7811 - acc: 0.56 - ETA: 24s - loss: 5.6508 - acc: 0.59 - ETA: 22s - loss: 6.2239 - acc: 0.56 - ETA: 21s - loss: 6.4389 - acc: 0.56 - ETA: 19s - loss: 6.4494 - acc: 0.56 - ETA: 18s - loss: 6.3635 - acc: 0.57 - ETA: 17s - loss: 6.4357 - acc: 0.57 - ETA: 16s - loss: 6.6187 - acc: 0.56 - ETA: 15s - loss: 6.4496 - acc: 0.57 - ETA: 14s - loss: 6.5488 - acc: 0.57 - ETA: 13s - loss: 6.6231 - acc: 0.56 - ETA: 11s - loss: 6.5436 - acc: 0.57 - ETA: 10s - loss: 6.4747 - acc: 0.58 - ETA: 9s - loss: 6.4301 - acc: 0.5850 - ETA: 8s - loss: 6.4791 - acc: 0.582 - ETA: 7s - loss: 6.5505 - acc: 0.579 - ETA: 6s - loss: 6.5814 - acc: 0.577 - ETA: 5s - loss: 6.5905 - acc: 0.577 - ETA: 4s - loss: 6.6226 - acc: 0.576 - ETA: 3s - loss: 6.6403 - acc: 0.575 - ETA: 2s - loss: 6.5857 - acc: 0.579 - ETA: 1s - loss: 6.6348 - acc: 0.576 - ETA: 0s - loss: 6.5598 - acc: 0.581 - 27s 17ms/step - loss: 6.5485 - acc: 0.5827 - val_loss: 6.8930 - val_acc: 0.5700\n",
      "Epoch 2/10\n",
      "1615/1615 [==============================] - ETA: 24s - loss: 7.5142 - acc: 0.53 - ETA: 23s - loss: 7.5142 - acc: 0.53 - ETA: 22s - loss: 7.4307 - acc: 0.53 - ETA: 21s - loss: 7.0132 - acc: 0.56 - ETA: 20s - loss: 6.9631 - acc: 0.56 - ETA: 19s - loss: 6.7233 - acc: 0.57 - ETA: 18s - loss: 6.6574 - acc: 0.58 - ETA: 17s - loss: 6.6705 - acc: 0.58 - ETA: 16s - loss: 6.6529 - acc: 0.58 - ETA: 15s - loss: 6.5637 - acc: 0.58 - ETA: 14s - loss: 6.5135 - acc: 0.59 - ETA: 13s - loss: 6.5134 - acc: 0.59 - ETA: 12s - loss: 6.7253 - acc: 0.57 - ETA: 11s - loss: 6.8353 - acc: 0.57 - ETA: 10s - loss: 6.7804 - acc: 0.57 - ETA: 9s - loss: 6.7480 - acc: 0.5786 - ETA: 8s - loss: 6.7194 - acc: 0.580 - ETA: 7s - loss: 6.6383 - acc: 0.585 - ETA: 6s - loss: 6.6580 - acc: 0.584 - ETA: 5s - loss: 6.7564 - acc: 0.578 - ETA: 4s - loss: 6.7448 - acc: 0.578 - ETA: 3s - loss: 6.7456 - acc: 0.578 - ETA: 2s - loss: 6.6701 - acc: 0.583 - ETA: 1s - loss: 6.6427 - acc: 0.585 - ETA: 0s - loss: 6.6475 - acc: 0.585 - 26s 16ms/step - loss: 6.6552 - acc: 0.5845 - val_loss: 6.8930 - val_acc: 0.5700\n",
      "Epoch 3/10\n",
      "1615/1615 [==============================] - ETA: 25s - loss: 7.0132 - acc: 0.56 - ETA: 25s - loss: 7.6394 - acc: 0.52 - ETA: 23s - loss: 7.0132 - acc: 0.56 - ETA: 22s - loss: 7.0758 - acc: 0.55 - ETA: 21s - loss: 6.9130 - acc: 0.56 - ETA: 20s - loss: 6.9297 - acc: 0.56 - ETA: 19s - loss: 6.9417 - acc: 0.56 - ETA: 18s - loss: 7.0446 - acc: 0.56 - ETA: 17s - loss: 6.9576 - acc: 0.56 - ETA: 16s - loss: 6.7377 - acc: 0.57 - ETA: 14s - loss: 6.6717 - acc: 0.58 - ETA: 13s - loss: 6.6793 - acc: 0.58 - ETA: 12s - loss: 6.7435 - acc: 0.57 - ETA: 11s - loss: 6.7628 - acc: 0.57 - ETA: 10s - loss: 6.6960 - acc: 0.58 - ETA: 9s - loss: 6.6069 - acc: 0.5869 - ETA: 8s - loss: 6.6308 - acc: 0.585 - ETA: 7s - loss: 6.4990 - acc: 0.593 - ETA: 6s - loss: 6.5656 - acc: 0.589 - ETA: 5s - loss: 6.6506 - acc: 0.584 - ETA: 4s - loss: 6.6321 - acc: 0.585 - ETA: 3s - loss: 6.6608 - acc: 0.583 - ETA: 2s - loss: 6.6761 - acc: 0.582 - ETA: 1s - loss: 6.6484 - acc: 0.584 - ETA: 0s - loss: 6.6630 - acc: 0.583 - 27s 16ms/step - loss: 6.6508 - acc: 0.5845 - val_loss: 6.8930 - val_acc: 0.5700\n",
      "Epoch 4/10\n",
      "1615/1615 [==============================] - ETA: 25s - loss: 5.5104 - acc: 0.65 - ETA: 24s - loss: 6.3870 - acc: 0.60 - ETA: 23s - loss: 6.8462 - acc: 0.57 - ETA: 22s - loss: 6.7001 - acc: 0.58 - ETA: 21s - loss: 6.9136 - acc: 0.56 - ETA: 20s - loss: 6.8779 - acc: 0.56 - ETA: 19s - loss: 6.9288 - acc: 0.56 - ETA: 18s - loss: 7.0372 - acc: 0.55 - ETA: 17s - loss: 6.5090 - acc: 0.54 - ETA: 15s - loss: 6.0388 - acc: 0.53 - ETA: 14s - loss: 6.1453 - acc: 0.53 - ETA: 13s - loss: 6.1667 - acc: 0.53 - ETA: 12s - loss: 5.7496 - acc: 0.53 - ETA: 11s - loss: 5.3891 - acc: 0.53 - ETA: 10s - loss: 5.0751 - acc: 0.54 - ETA: 9s - loss: 4.8010 - acc: 0.5405 - ETA: 8s - loss: 4.5589 - acc: 0.536 - ETA: 7s - loss: 4.3470 - acc: 0.535 - ETA: 6s - loss: 4.1541 - acc: 0.537 - ETA: 5s - loss: 3.9786 - acc: 0.539 - ETA: 4s - loss: 3.8217 - acc: 0.539 - ETA: 3s - loss: 3.6768 - acc: 0.543 - ETA: 2s - loss: 3.5461 - acc: 0.545 - ETA: 1s - loss: 3.4285 - acc: 0.545 - ETA: 0s - loss: 3.3181 - acc: 0.548 - 29s 18ms/step - loss: 3.2923 - acc: 0.5495 - val_loss: 2.2425 - val_acc: 0.5700\n",
      "Epoch 5/10\n",
      "1615/1615 [==============================] - ETA: 35s - loss: 2.3047 - acc: 0.52 - ETA: 32s - loss: 1.5083 - acc: 0.48 - ETA: 31s - loss: 1.2266 - acc: 0.53 - ETA: 29s - loss: 1.0799 - acc: 0.56 - ETA: 28s - loss: 0.9967 - acc: 0.57 - ETA: 26s - loss: 0.9448 - acc: 0.58 - ETA: 25s - loss: 0.9019 - acc: 0.58 - ETA: 23s - loss: 0.8702 - acc: 0.59 - ETA: 22s - loss: 0.8525 - acc: 0.58 - ETA: 21s - loss: 0.8336 - acc: 0.58 - ETA: 19s - loss: 0.8191 - acc: 0.58 - ETA: 17s - loss: 0.8061 - acc: 0.58 - ETA: 16s - loss: 0.7983 - acc: 0.58 - ETA: 15s - loss: 0.7916 - acc: 0.57 - ETA: 13s - loss: 0.7826 - acc: 0.58 - ETA: 12s - loss: 0.7794 - acc: 0.58 - ETA: 10s - loss: 0.7742 - acc: 0.57 - ETA: 9s - loss: 0.7688 - acc: 0.5734 - ETA: 7s - loss: 0.7631 - acc: 0.576 - ETA: 6s - loss: 0.7588 - acc: 0.578 - ETA: 5s - loss: 0.7552 - acc: 0.578 - ETA: 4s - loss: 0.7528 - acc: 0.576 - ETA: 2s - loss: 0.7487 - acc: 0.578 - ETA: 1s - loss: 0.7474 - acc: 0.580 - ETA: 0s - loss: 0.7438 - acc: 0.580 - 32s 20ms/step - loss: 0.7423 - acc: 0.5833 - val_loss: 1.9191 - val_acc: 0.5700\n",
      "Epoch 6/10\n",
      "1615/1615 [==============================] - ETA: 25s - loss: 2.2257 - acc: 0.47 - ETA: 25s - loss: 1.4605 - acc: 0.50 - ETA: 23s - loss: 1.2386 - acc: 0.50 - ETA: 24s - loss: 1.0902 - acc: 0.53 - ETA: 24s - loss: 1.0055 - acc: 0.53 - ETA: 24s - loss: 0.9488 - acc: 0.54 - ETA: 23s - loss: 0.9062 - acc: 0.55 - ETA: 23s - loss: 0.8803 - acc: 0.56 - ETA: 22s - loss: 0.8567 - acc: 0.56 - ETA: 20s - loss: 0.8381 - acc: 0.56 - ETA: 19s - loss: 0.8255 - acc: 0.56 - ETA: 18s - loss: 0.8136 - acc: 0.56 - ETA: 16s - loss: 0.8010 - acc: 0.56 - ETA: 15s - loss: 0.7874 - acc: 0.57 - ETA: 14s - loss: 0.7779 - acc: 0.57 - ETA: 12s - loss: 0.7683 - acc: 0.58 - ETA: 11s - loss: 0.7623 - acc: 0.58 - ETA: 9s - loss: 0.7573 - acc: 0.5881 - ETA: 8s - loss: 0.7658 - acc: 0.583 - ETA: 7s - loss: 0.7633 - acc: 0.577 - ETA: 5s - loss: 0.7593 - acc: 0.577 - ETA: 4s - loss: 0.7562 - acc: 0.573 - ETA: 3s - loss: 0.7519 - acc: 0.575 - ETA: 1s - loss: 0.7472 - acc: 0.577 - ETA: 0s - loss: 0.7440 - acc: 0.577 - 35s 22ms/step - loss: 0.7429 - acc: 0.5783 - val_loss: 0.6821 - val_acc: 0.5600\n",
      "Epoch 7/10\n",
      "1615/1615 [==============================] - ETA: 30s - loss: 0.5971 - acc: 0.68 - ETA: 28s - loss: 0.6023 - acc: 0.67 - ETA: 28s - loss: 0.6473 - acc: 0.63 - ETA: 27s - loss: 0.6406 - acc: 0.65 - ETA: 26s - loss: 0.6399 - acc: 0.64 - ETA: 25s - loss: 0.6383 - acc: 0.63 - ETA: 23s - loss: 0.6420 - acc: 0.62 - ETA: 22s - loss: 0.6441 - acc: 0.62 - ETA: 20s - loss: 0.6546 - acc: 0.61 - ETA: 19s - loss: 0.6628 - acc: 0.60 - ETA: 18s - loss: 0.6538 - acc: 0.61 - ETA: 16s - loss: 0.6641 - acc: 0.60 - ETA: 15s - loss: 0.6666 - acc: 0.60 - ETA: 14s - loss: 0.6688 - acc: 0.59 - ETA: 12s - loss: 0.6763 - acc: 0.59 - ETA: 11s - loss: 0.6758 - acc: 0.59 - ETA: 10s - loss: 0.6756 - acc: 0.59 - ETA: 9s - loss: 0.6743 - acc: 0.5964 - ETA: 7s - loss: 0.6736 - acc: 0.596 - ETA: 6s - loss: 0.6763 - acc: 0.594 - ETA: 5s - loss: 0.6795 - acc: 0.591 - ETA: 4s - loss: 0.6791 - acc: 0.592 - ETA: 2s - loss: 0.6764 - acc: 0.595 - ETA: 1s - loss: 0.6740 - acc: 0.600 - ETA: 0s - loss: 0.6722 - acc: 0.604 - 32s 20ms/step - loss: 0.6760 - acc: 0.6025 - val_loss: 0.8407 - val_acc: 0.4450\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615/1615 [==============================] - ETA: 28s - loss: 0.7983 - acc: 0.58 - ETA: 27s - loss: 1.8284 - acc: 0.58 - ETA: 26s - loss: 1.6283 - acc: 0.56 - ETA: 25s - loss: 1.3848 - acc: 0.57 - ETA: 24s - loss: 1.2408 - acc: 0.57 - ETA: 22s - loss: 1.1447 - acc: 0.57 - ETA: 21s - loss: 1.0668 - acc: 0.59 - ETA: 20s - loss: 1.0168 - acc: 0.59 - ETA: 19s - loss: 0.9760 - acc: 0.59 - ETA: 18s - loss: 0.9397 - acc: 0.60 - ETA: 17s - loss: 0.9175 - acc: 0.60 - ETA: 16s - loss: 0.8918 - acc: 0.60 - ETA: 15s - loss: 0.8728 - acc: 0.61 - ETA: 13s - loss: 0.8562 - acc: 0.61 - ETA: 12s - loss: 0.8413 - acc: 0.61 - ETA: 11s - loss: 0.8273 - acc: 0.61 - ETA: 10s - loss: 0.8153 - acc: 0.61 - ETA: 8s - loss: 0.8020 - acc: 0.6241 - ETA: 7s - loss: 0.8045 - acc: 0.622 - ETA: 6s - loss: 0.7957 - acc: 0.624 - ETA: 5s - loss: 0.7879 - acc: 0.625 - ETA: 3s - loss: 0.7800 - acc: 0.624 - ETA: 2s - loss: 0.7752 - acc: 0.625 - ETA: 1s - loss: 0.7700 - acc: 0.623 - ETA: 0s - loss: 0.7654 - acc: 0.622 - 31s 19ms/step - loss: 0.7643 - acc: 0.6229 - val_loss: 0.7234 - val_acc: 0.5200\n",
      "Epoch 9/10\n",
      "1615/1615 [==============================] - ETA: 29s - loss: 0.7007 - acc: 0.53 - ETA: 29s - loss: 0.6522 - acc: 0.60 - ETA: 28s - loss: 0.6253 - acc: 0.61 - ETA: 25s - loss: 0.6151 - acc: 0.63 - ETA: 23s - loss: 0.6040 - acc: 0.65 - ETA: 22s - loss: 0.6067 - acc: 0.64 - ETA: 20s - loss: 0.6274 - acc: 0.62 - ETA: 19s - loss: 0.6221 - acc: 0.62 - ETA: 18s - loss: 0.6113 - acc: 0.63 - ETA: 16s - loss: 0.6448 - acc: 0.63 - ETA: 15s - loss: 0.6489 - acc: 0.62 - ETA: 14s - loss: 0.6437 - acc: 0.63 - ETA: 13s - loss: 0.6350 - acc: 0.64 - ETA: 12s - loss: 0.6363 - acc: 0.64 - ETA: 11s - loss: 0.6330 - acc: 0.64 - ETA: 10s - loss: 0.6353 - acc: 0.64 - ETA: 9s - loss: 0.6303 - acc: 0.6489 - ETA: 8s - loss: 0.6271 - acc: 0.654 - ETA: 6s - loss: 0.6255 - acc: 0.655 - ETA: 5s - loss: 0.6258 - acc: 0.651 - ETA: 4s - loss: 0.6281 - acc: 0.646 - ETA: 3s - loss: 0.6269 - acc: 0.649 - ETA: 2s - loss: 0.6327 - acc: 0.644 - ETA: 1s - loss: 0.6324 - acc: 0.644 - ETA: 0s - loss: 0.6320 - acc: 0.643 - 29s 18ms/step - loss: 0.6309 - acc: 0.6452 - val_loss: 0.6719 - val_acc: 0.6050\n",
      "Epoch 10/10\n",
      "1615/1615 [==============================] - ETA: 27s - loss: 0.5507 - acc: 0.74 - ETA: 24s - loss: 0.5629 - acc: 0.69 - ETA: 23s - loss: 0.5666 - acc: 0.70 - ETA: 22s - loss: 0.5638 - acc: 0.69 - ETA: 20s - loss: 0.5830 - acc: 0.68 - ETA: 19s - loss: 0.5874 - acc: 0.67 - ETA: 18s - loss: 0.5855 - acc: 0.67 - ETA: 17s - loss: 0.5933 - acc: 0.67 - ETA: 16s - loss: 0.5806 - acc: 0.67 - ETA: 15s - loss: 0.5913 - acc: 0.67 - ETA: 14s - loss: 0.5888 - acc: 0.67 - ETA: 13s - loss: 0.5869 - acc: 0.67 - ETA: 12s - loss: 0.5831 - acc: 0.67 - ETA: 11s - loss: 0.5871 - acc: 0.67 - ETA: 10s - loss: 0.5854 - acc: 0.67 - ETA: 9s - loss: 0.5903 - acc: 0.6738 - ETA: 8s - loss: 0.5900 - acc: 0.671 - ETA: 7s - loss: 0.5887 - acc: 0.671 - ETA: 6s - loss: 0.5883 - acc: 0.671 - ETA: 5s - loss: 0.5883 - acc: 0.670 - ETA: 4s - loss: 0.5850 - acc: 0.675 - ETA: 3s - loss: 0.5799 - acc: 0.679 - ETA: 2s - loss: 0.5817 - acc: 0.678 - ETA: 1s - loss: 0.5911 - acc: 0.677 - ETA: 0s - loss: 0.5933 - acc: 0.675 - 27s 17ms/step - loss: 0.5922 - acc: 0.6774 - val_loss: 0.7629 - val_acc: 0.5700\n"
     ]
    }
   ],
   "source": [
    "binary_train = model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
